import collections
import gym
import numpy as np
import statistics
import tensorflow as tf
print(tf.__version__)
import tqdm

from matplotlib import pyplot as plt
from tensorflow.keras import layers
from typing import Any, List, Sequence, Tuple

# for diaplay
from IPython import display as ipythondisplay
from PIL import Image
from pyvirtualdisplay import Display



# Create the environment
env = gym.make("CartPole-v0")

# Set seed for experiment reproducibility
seed = 42
env.seed(seed)
tf.random.set_seed(seed)
np.random.seed(seed)

# Small epsilon value for stabilizing division operations
eps = np.finfo(np.float32).eps.item()
print (eps)



# Here Policy is referred to as the actor that proposes a set of possible actions given a state.
# The estimated value function is referred to as the critic which evaluates actions taken by the actor based on the given policy.


#The Actor and Critic will be modeled using one neural network with two outputs that generates the action probabilities and critic value respectively. 
# Later we use model subclassing to define the model.

# During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value  V , which models the state-dependent value function.

#####   The goal is to train a model that chooses actions based on a policy  π  that maximizes expected return.

###  There are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. 
#### The agent can take two actions to push the cart left (0) and right (1) respectively.

class ActorCritic(tf.keras.Model):
  """Combined actor-critic network."""

  def __init__(
      self, 
      num_actions: int, 
      num_hidden_units: int):
    """Initialize."""
    super().__init__()

    self.common = layers.Dense(num_hidden_units, activation="relu")
    self.actor = layers.Dense(num_actions)
    self.critic = layers.Dense(1)

  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
    x = self.common(inputs)
    return self.actor(x), self.critic(x)


num_actions = env.action_space.n  # 2
num_hidden_units = 128

model = ActorCritic(num_actions, num_hidden_units)


# Collecting training data
## As in supervised learning, in order to train the actor-critic model, you need to have training data. However, 
# in order to collect such data, the model would need to be "run" in the environment.


# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.
# This would allow it to be included in a callable TensorFlow graph.
def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
  """Returns state, reward and done flag given an action."""

  state, reward, done, _ = env.step(action)
  return (state.astype(np.float32), 
          np.array(reward, np.int32), 
          np.array(done, np.int32))


def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:
  return tf.numpy_function(env_step, [action], 
                           [tf.float32, tf.int32, tf.int32])

#As in supervised learning, in order to train the actor-critic model, you need to have training data. However, in order to collect such data, 
# the model would need to be "run" in the environment.

#Training data is collected for each episode. Then at each time step, the model's forward pass will be run on the environment's state in order 
# to generate action probabilities and the critic value based on the current policy parameterized by the model's weights.

#The next action will be sampled from the action probabilities generated by the model, which would then be applied to the environment, 
# causing the next state and reward to be generated.

#This process is implemented in the run_episode function, which uses TensorFlow operations so that it can later be compiled into a 
# TensorFlow graph for faster training. Note that tf.TensorArrays were used to support Tensor iteration on variable length arrays.

def run_episode(
    initial_state: tf.Tensor,  
    model: tf.keras.Model, 
    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
  """Runs a single episode to collect training data."""

  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)

  initial_state_shape = initial_state.shape
  state = initial_state

  for t in tf.range(max_steps):
    # Convert state into a batched tensor (batch size = 1)
    state = tf.expand_dims(state, 0)
  
    # Run the model and to get action probabilities and critic value
    action_logits_t, value = model(state)
  
    # Sample next action from the action probability distribution
    action = tf.random.categorical(action_logits_t, 1)[0, 0]
    action_probs_t = tf.nn.softmax(action_logits_t)

    # Store critic values
    values = values.write(t, tf.squeeze(value))

    # Store log probability of the action chosen
    action_probs = action_probs.write(t, action_probs_t[0, action])
  
    # Apply action to the environment to get next state and reward
    state, reward, done = tf_env_step(action)
    state.set_shape(initial_state_shape)
  
    # Store reward
    rewards = rewards.write(t, reward)

    if tf.cast(done, tf.bool):
      break

  action_probs = action_probs.stack()
  values = values.stack()
  rewards = rewards.stack()
  
  return action_probs, values, rewards


## Computing expected returns
## The sequence of rewards for each timestep  t ,  collected during one episode is converted into a sequence of expected returns 
#  in which the sum of rewards is taken from the current timestep  t  to  T  and each reward is multiplied with an exponentially decaying discount factor  γ :
## Since  γ∈(0,1) , rewards further out from the current timestep are given less weight.
## Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.
## To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).

def get_expected_return(
    rewards: tf.Tensor, 
    gamma: float, 
    standardize: bool = True) -> tf.Tensor:
  """Compute expected returns per timestep."""

  n = tf.shape(rewards)[0]
  returns = tf.TensorArray(dtype=tf.float32, size=n)

  # Start from the end of `rewards` and accumulate reward sums
  # into the `returns` array
  rewards = tf.cast(rewards[::-1], dtype=tf.float32)
  discounted_sum = tf.constant(0.0)
  discounted_sum_shape = discounted_sum.shape
  for i in tf.range(n):
    reward = rewards[i]
    discounted_sum = reward + gamma * discounted_sum
    discounted_sum.set_shape(discounted_sum_shape)
    returns = returns.write(i, discounted_sum)
  returns = returns.stack()[::-1]

  if standardize:
    returns = ((returns - tf.math.reduce_mean(returns)) / 
               (tf.math.reduce_std(returns) + eps))

  return returns


### The actor-critic loss
## Since a hybrid actor-critic model is used, the chosen loss function is a combination of actor and critic losses for training, as shown below:

##                                  L=Loss(actor) + Loss (critic) 

## Actor loss (policy is referred to as the actor that proposes a set of possible actions given a state,)
# The estimated value function is referred to as the critic, which evaluates actions taken by the actor based on the given policy.
## The actor loss is based on policy gradients with the critic as a state dependent baseline and computed with single-sample (per-episode) estimates.

huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)

def compute_loss(
    action_probs: tf.Tensor,  
    values: tf.Tensor,  
    returns: tf.Tensor) -> tf.Tensor:
  """Computes the combined actor-critic loss."""
  # Advantage - The  G−V  term in our  Loss (actor)  formulation is called the advantage, 
  #   G = the expected return for a given state, action pair at timestep  t
  #   V =  the value function (critic) also parameterized by  θ
  # which indicates how much better an action is given a particular state over a random action selected according to the policy  π  for that state.
  advantage = returns - values

  action_log_probs = tf.math.log(action_probs)
  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)

  ### Critic loss - Training  V  to be as close possible to  G  can be set up as a regression problem with the following loss function:
  #   Loss(critic)=Lδ(G,Vπθ) 
  # where  Lδ  is the Huber loss, which is less sensitive to outliers in data than squared-error loss.

  critic_loss = huber_loss(values, returns)

  return actor_loss + critic_loss





## The training step to update parameters
# All of the steps above are combined into a training step that is run every episode. All steps leading up to the loss function are 
# executed with the tf.GradientTape context to enable automatic differentiation.
# This program uses the Adam optimizer to apply the gradients to the model parameters.
# The sum of the undiscounted rewards, episode_reward, is also computed in this step. This value will be used later on to evaluate if the success criterion is met.
# The tf.function context is applied to the train_step function so that it can be compiled into a callable TensorFlow graph, which can lead to 10x speedup in training.

optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
#tf. function is a decorator function provided by Tensorflow 2.0 that converts regular python code to a callable Tensorflow graph 
# function, which is usually more performant and python independent. It is used to create portable Tensorflow models
@tf.function
def train_step(
    initial_state: tf.Tensor, 
    model: tf.keras.Model, 
    optimizer: tf.keras.optimizers.Optimizer, 
    gamma: float, 
    max_steps_per_episode: int) -> tf.Tensor:
  """Runs a model training step."""

  with tf.GradientTape() as tape:

    # Run the model for one episode to collect training data
    action_probs, values, rewards = run_episode(
        initial_state, model, max_steps_per_episode) 

    # Calculate expected returns
    returns = get_expected_return(rewards, gamma)

    # Convert training data to appropriate TF tensor shapes
    action_probs, values, returns = [
        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] 

    # Calculating loss values to update our network
    loss = compute_loss(action_probs, values, returns)

  # Compute the gradients from the loss
  grads = tape.gradient(loss, model.trainable_variables)

  # Apply the gradients to the model's parameters
  optimizer.apply_gradients(zip(grads, model.trainable_variables))

  episode_reward = tf.math.reduce_sum(rewards)

  return episode_reward



## Training
# To train the agent, you will follow these steps:
# Run the agent on the environment to collect training data per episode.
# Compute expected return at each time step.
# Compute the loss for the combined actor-critic model.
# Compute gradients and update network parameters.
# Repeat 1-4 until either success criterion or max episodes has been reached.


#Run the training loop
#Training is executed by running the training step until either the success criterion or maximum number of episodes is reached.

#A running record of episode rewards is kept in a queue. Once 100 trials are reached, the oldest reward is removed at the left (tail) end of the queue and the newest one is added at the head (right). A running sum of the rewards is also maintained for computational efficiency.

#Depending on your runtime, training can finish in less than a minute.
 
min_episodes_criterion = 100
max_episodes = 10000
max_steps_per_episode = 1000

# Cartpole-v0 is considered solved if average reward is >= 195 over 100 
# consecutive trials
reward_threshold = 195
running_reward = 0

# Discount factor for future rewards
gamma = 0.99

# Keep last episodes reward
episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)
#tqdm is Python library that allows you to output a smart progress bar by wrapping around any iterable
#A tqdm progress bar not only shows you how much time has elapsed, but also shows the estimated time remaining for the iterable.
with tqdm.trange(max_episodes) as t:
  for i in t:
    initial_state = tf.constant(env.reset(), dtype=tf.float32)
    episode_reward = int(train_step(
        initial_state, model, optimizer, gamma, max_steps_per_episode))
    
    episodes_reward.append(episode_reward)
    running_reward = statistics.mean(episodes_reward)
  
    t.set_description(f'Episode {i}')
    t.set_postfix(
        episode_reward=episode_reward, running_reward=running_reward)
  
    # Show average episode reward every 10 episodes
    if i % 10 == 0:
      pass # print(f'Episode {i}: average reward: {avg_reward}')
  
    if running_reward > reward_threshold and i >= min_episodes_criterion:  
        break

print(f'\nSolved at episode {i}: average reward: {running_reward:.2f}!')

## Visualization
## After training, it would be good to visualize how the model performs in the environment. You can run the cells below to generate a GIF animation of one
#  episode run of the model. Note that additional packages need to be installed for OpenAI Gym to render the environment's images correctly in Colab.

# Render an episode and save as a GIF file




display = Display(visible=0, size=(400, 300))
display.start()


def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): 
  screen = env.render(mode='rgb_array')
  im = Image.fromarray(screen)

  images = [im]
  
  state = tf.constant(env.reset(), dtype=tf.float32)
  for i in range(1, max_steps + 1):
    state = tf.expand_dims(state, 0)
    action_probs, _ = model(state)
    action = np.argmax(np.squeeze(action_probs))

    state, _, done, _ = env.step(action)
    state = tf.constant(state, dtype=tf.float32)

    # Render screen every 10 steps
    if i % 10 == 0:
      screen = env.render(mode='rgb_array')
      images.append(Image.fromarray(screen))
  
    if done:
      break
  
  return images


# Save GIF image
images = render_episode(env, model, max_steps_per_episode)
image_file = 'cartpole-v0.gif'
# loop=0: loop forever, duration=1: play each frame for 1ms
images[0].save(
    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)

#import tensorflow_docs.vis.embed as embed
#embed.embed_file(image_file)